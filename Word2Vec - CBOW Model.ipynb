{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29e6546a-2cf5-413a-a41b-9804200bea97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: adjustText in c:\\users\\ariji\\anaconda3\\lib\\site-packages (1.2.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\ariji\\anaconda3\\lib\\site-packages (from adjustText) (1.24.4)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\ariji\\anaconda3\\lib\\site-packages (from adjustText) (3.8.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\ariji\\anaconda3\\lib\\site-packages (from adjustText) (1.10.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\ariji\\anaconda3\\lib\\site-packages (from matplotlib->adjustText) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\ariji\\anaconda3\\lib\\site-packages (from matplotlib->adjustText) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\ariji\\anaconda3\\lib\\site-packages (from matplotlib->adjustText) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\ariji\\anaconda3\\lib\\site-packages (from matplotlib->adjustText) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ariji\\anaconda3\\lib\\site-packages (from matplotlib->adjustText) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\ariji\\anaconda3\\lib\\site-packages (from matplotlib->adjustText) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\ariji\\anaconda3\\lib\\site-packages (from matplotlib->adjustText) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\ariji\\anaconda3\\lib\\site-packages (from matplotlib->adjustText) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ariji\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->adjustText) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install adjustText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "943a12ef-ace7-4303-904d-f09ab95aaae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them before proceeding further.\n",
    "%matplotlib inline\n",
    "import zipfile\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='3'\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from sklearn.manifold import TSNE\n",
    "from adjustText import adjust_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295bde51-2a42-4d06-9e93-77bc456d99ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://mlg.ucd.ie/files/datasets/bbc-fulltext.zip'\n",
    "\n",
    "\n",
    "def download_data(url, data_dir):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "    file_path = os.path.join(data_dir, 'bbc-fulltext.zip')\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        print('Downloading file...')\n",
    "        filename, _ = urlretrieve(url, file_path)\n",
    "    else:\n",
    "        print(\"File already exists\")\n",
    "\n",
    "    extract_path = os.path.join(data_dir, 'bbc')\n",
    "    if not os.path.exists(extract_path):\n",
    "\n",
    "        with zipfile.ZipFile(os.path.join(data_dir, 'bbc-fulltext.zip'), 'r') as zipf:\n",
    "            zipf.extractall(data_dir)\n",
    "\n",
    "    else:\n",
    "        print(\"bbc-fulltext.zip has already been extracted\")\n",
    "\n",
    "download_data(url, 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "762f9fd2-5d38-40d8-86a5-d4d4fad26e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading files\n",
      "................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................. 401.txt\n",
      "Detected 2225 stories\n",
      "865163 words found in the total news set\n",
      "Example words (start):  Ad sales boost Time Warner profit  Quarterly profi\n",
      "Example words (end):  Online was the game, ahhhh them was the days ! LOL\n"
     ]
    }
   ],
   "source": [
    "def read_data(data_dir):\n",
    "\n",
    "    # This will contain the full list of stories\n",
    "    news_stories = []\n",
    "\n",
    "    print(\"Reading files\")\n",
    "\n",
    "    i = 0 # Just used for printing progress\n",
    "    for root, dirs, files in os.walk(data_dir):\n",
    "\n",
    "        for fi, f in enumerate(files):\n",
    "\n",
    "            # We don't read the README file\n",
    "            if 'README' in f:\n",
    "                continue\n",
    "\n",
    "            # Printing progress\n",
    "            i += 1\n",
    "            print(\".\"*i, f, end='\\r')\n",
    "\n",
    "            # Open the file\n",
    "            with open(os.path.join(root, f), encoding='latin-1') as f:\n",
    "\n",
    "                story = []\n",
    "                # Read all the lines\n",
    "                for row in f:\n",
    "\n",
    "                    story.append(row.strip())\n",
    "\n",
    "                # Create a single string with all the rows in the doc\n",
    "                story = ' '.join(story)\n",
    "                # Add that to the list\n",
    "                news_stories.append(story)\n",
    "\n",
    "        print('', end='\\r')\n",
    "\n",
    "    print(f\"\\nDetected {len(news_stories)} stories\")\n",
    "    return news_stories\n",
    "\n",
    "\n",
    "news_stories = read_data(os.path.join('data', 'bbc'))\n",
    "\n",
    "# Printing some stats and sample data\n",
    "print(f\"{sum([len(story.split(' ')) for story in news_stories])} words found in the total news set\")\n",
    "print('Example words (start): ',news_stories[0][:50])\n",
    "print('Example words (end): ',news_stories[-1][-50:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "42137e80-fbd0-441b-a2ff-d3a7c93cdac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data fitted on the tokenizer\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(\n",
    "    num_words=None,\n",
    "    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "    lower=True, split=' '\n",
    ")\n",
    "\n",
    "tokenizer.fit_on_texts(news_stories)\n",
    "print(\"Data fitted on the tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "55ea0a1e-5395-4579-8cd4-e0ad99b8efdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 32360\n",
      "\n",
      "Words at the top\n",
      "\t {'the': 1, 'to': 2, 'of': 3, 'and': 4, 'a': 5, 'in': 6, 'for': 7, 'is': 8, 'that': 9, 'on': 10}\n",
      "\n",
      "Words at the bottom\n",
      "\t {'counsellor': 32350, \"'frag'\": 32351, 'relasing': 32352, \"'real'\": 32353, 'hrs': 32354, 'enviroment': 32355, 'trifling': 32356, '24hours': 32357, 'ahhhh': 32358, 'lol': 32359}\n"
     ]
    }
   ],
   "source": [
    "n_vocab = len(tokenizer.word_index.items()) + 1\n",
    "print(f\"Vocabulary size: {n_vocab}\")\n",
    "\n",
    "print(\"\\nWords at the top\")\n",
    "print('\\t', dict(list(tokenizer.word_index.items())[:10]))\n",
    "print(\"\\nWords at the bottom\")\n",
    "print('\\t', dict(list(tokenizer.word_index.items())[-10:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "94e4f7ec-851f-45c2-a2e2-9f20e12e479d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data fitted on the tokenizer\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "n_vocab = 15000 + 1\n",
    "tokenizer = Tokenizer(\n",
    "    num_words=n_vocab-1,\n",
    "    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "    lower=True, split=' ', oov_token='',\n",
    ")\n",
    "\n",
    "tokenizer.fit_on_texts(news_stories)\n",
    "print(\"Data fitted on the tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1180e586-17a8-4ab7-9171-88e857efa51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_sequences = tokenizer.texts_to_sequences(news_stories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ed581edc-09e7-48a7-88d5-de720f88c410",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(n_vocab, sampling_factor=1e-05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6175b221-87b9-4abb-8fd2-2c70947ba66d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[28, 428, 132, 3, 2, 1505]]\n",
      "Input: (3, [132, 2]) ([['going', 'the'], 'to'])/ Label: 1\n",
      "Input: (58, [3, 1505]) ([['to', 'store'], 'can'])/ Label: 0\n",
      "Input: (540, [132, 2]) ([['going', 'the'], 'asked'])/ Label: 0\n",
      "Input: (132, [428, 3]) ([['am', 'to'], 'going'])/ Label: 1\n",
      "Input: (399, [3, 1505]) ([['to', 'store'], 'major'])/ Label: 0\n",
      "Input: (671, [132, 2]) ([['going', 'the'], '17'])/ Label: 0\n",
      "Input: (387, [28, 132]) ([['i', 'going'], 'support'])/ Label: 0\n",
      "Input: (2, [428, 3]) ([['am', 'to'], 'the'])/ Label: 0\n",
      "Input: (22, [132, 2]) ([['going', 'the'], 'by'])/ Label: 0\n",
      "Input: (106, [3, 1505]) ([['to', 'store'], 'while'])/ Label: 0\n",
      "Input: (9284, [28, 132]) ([['i', 'going'], 'bulletin'])/ Label: 0\n",
      "Input: (25, [132, 2]) ([['going', 'the'], 'are'])/ Label: 0\n",
      "Input: (31, [428, 3]) ([['am', 'to'], 'they'])/ Label: 0\n",
      "Input: (105, [428, 3]) ([['am', 'to'], 'her'])/ Label: 0\n",
      "Input: (428, [28, 132]) ([['i', 'going'], 'am'])/ Label: 1\n",
      "Input: (7669, [28, 132]) ([['i', 'going'], 'mogul'])/ Label: 0\n",
      "Input: (15845, [28, 132]) ([['i', 'going'], 'bussereau'])/ Label: 0\n",
      "Input: (10413, [3, 1505]) ([['to', 'store'], 'dumb'])/ Label: 0\n",
      "Input: (2, [3, 1505]) ([['to', 'store'], 'the'])/ Label: 1\n",
      "Input: (16569, [428, 3]) ([['am', 'to'], 'inspected'])/ Label: 0\n"
     ]
    }
   ],
   "source": [
    "def cbow_grams(sequence, vocabulary_size,\n",
    "              window_size=4, negative_samples=1., shuffle=True,\n",
    "              categorical=False, sampling_table=None, seed=None):\n",
    "\n",
    "    targets, contexts, labels = [], [], []\n",
    "\n",
    "    for i, wi in enumerate(sequence):\n",
    "\n",
    "\n",
    "        if not wi or i < window_size or i + 1 > len(sequence)-window_size:\n",
    "            continue\n",
    "        if sampling_table is not None:\n",
    "            if sampling_table[wi] < random.random():\n",
    "                continue\n",
    "\n",
    "        window_start = max(0, i - window_size)\n",
    "        window_end = min(len(sequence), i + window_size + 1)\n",
    "\n",
    "        context_words = [wj for j, wj in enumerate(sequence[window_start:window_end]) if j+window_start != i]\n",
    "        target_word = wi\n",
    "\n",
    "        context_classes = tf.expand_dims(tf.constant(context_words, dtype=\"int64\"), 0)\n",
    "\n",
    "        negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
    "          true_classes=context_classes,\n",
    "          num_true=window_size * 2,\n",
    "          num_sampled=negative_samples,\n",
    "          unique=True,\n",
    "          range_max=vocabulary_size,\n",
    "          name=\"negative_sampling\")\n",
    "\n",
    "        # Build context and label vectors (for one target word)\n",
    "        negative_targets = negative_sampling_candidates.numpy().tolist()\n",
    "\n",
    "        target = [target_word] + negative_targets\n",
    "        label = [1] + [0]*negative_samples\n",
    "\n",
    "        # Append each element from the training example to global lists.\n",
    "        targets.extend(target)\n",
    "        contexts.extend([context_words]*(negative_samples+1))\n",
    "        labels.extend(label)\n",
    "\n",
    "    couples = list(zip(targets, contexts))\n",
    "\n",
    "    seed = random.randint(0, 10e6)\n",
    "    random.seed(seed)\n",
    "    random.shuffle(couples)\n",
    "    random.seed(seed)\n",
    "    random.shuffle(labels)\n",
    "\n",
    "\n",
    "    return couples, labels\n",
    "\n",
    "\n",
    "window_size = 1 # How many words to consider left and right.\n",
    "\n",
    "\n",
    "inputs, labels = cbow_grams(\n",
    "    tokenizer.texts_to_sequences([\"I am going to the store\"])[0],\n",
    "    vocabulary_size=len(tokenizer.word_index.items())+1,\n",
    "    window_size=window_size, negative_samples=4, shuffle=False,\n",
    "    categorical=False, sampling_table=None, seed=None\n",
    ")\n",
    "\n",
    "print(tokenizer.texts_to_sequences([\"I am going to the store\"]))\n",
    "i = 0\n",
    "for inp, lbl in zip(inputs, labels):\n",
    "    i += 1\n",
    "    print(f\"Input: {inp} ({[[tokenizer.index_word[wi] for wi in inp[1] ]] + [tokenizer.index_word[inp[0]] if inp[0] > 0 else None]})/ Label: {lbl}\")\n",
    "    #\n",
    "    if i >= 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3ea08d4a-6603-4eea-9fd9-83769a5c7eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [4223, 187] (['ad', 'sales']) / Label: 1\n",
      "Input: [187, 4223] (['sales', 'ad']) / Label: 1\n",
      "Input: [187, 716] (['sales', 'boost']) / Label: 1\n",
      "Input: [716, 187] (['boost', 'sales']) / Label: 1\n",
      "Input: [716, 66] (['boost', 'time']) / Label: 1\n",
      "Input: [66, 716] (['time', 'boost']) / Label: 1\n",
      "Input: [66, 3596] (['time', 'warner']) / Label: 1\n",
      "Input: [3596, 66] (['warner', 'time']) / Label: 1\n",
      "Input: [3596, 1050] (['warner', 'profit']) / Label: 1\n",
      "Input: [1050, 3596] (['profit', 'warner']) / Label: 1\n",
      "Input: [1050, 3938] (['profit', 'quarterly']) / Label: 1\n",
      "Input: [3938, 1050] (['quarterly', 'profit']) / Label: 1\n",
      "Input: [3938, 626] (['quarterly', 'profits']) / Label: 1\n",
      "Input: [626, 3938] (['profits', 'quarterly']) / Label: 1\n",
      "Input: [626, 21] (['profits', 'at']) / Label: 1\n",
      "Input: [21, 626] (['at', 'profits']) / Label: 1\n",
      "Input: [21, 49] (['at', 'us']) / Label: 1\n",
      "Input: [49, 21] (['us', 'at']) / Label: 1\n",
      "Input: [49, 303] (['us', 'media']) / Label: 1\n",
      "Input: [303, 49] (['media', 'us']) / Label: 1\n",
      "Input: [303, 717] (['media', 'giant']) / Label: 1\n",
      "Input: [717, 303] (['giant', 'media']) / Label: 1\n",
      "Input: [717, 8263] (['giant', 'timewarner']) / Label: 1\n",
      "Input: [8263, 717] (['timewarner', 'giant']) / Label: 1\n",
      "Input: [8263, 2972] (['timewarner', 'jumped']) / Label: 1\n",
      "Input: [2972, 8263] (['jumped', 'timewarner']) / Label: 1\n",
      "Input: [2972, 5321] (['jumped', '76']) / Label: 1\n",
      "Input: [5321, 2972] (['76', 'jumped']) / Label: 1\n",
      "Input: [5321, 3] (['76', 'to']) / Label: 1\n",
      "Input: [3, 5321] (['to', '76']) / Label: 1\n",
      "Input: [3, 108] (['to', '1']) / Label: 1\n",
      "Input: [108, 3] (['1', 'to']) / Label: 1\n",
      "Input: [108, 11185] (['1', '13bn']) / Label: 1\n",
      "Input: [11185, 108] (['13bn', '1']) / Label: 1\n",
      "Input: [11185, 1] (['13bn', '']) / Label: 1\n",
      "Input: [1, 11185] (['', '13bn']) / Label: 1\n",
      "Input: [1, 8] (['', 'for']) / Label: 1\n",
      "Input: [8, 1] (['for', '']) / Label: 1\n",
      "Input: [8, 2] (['for', 'the']) / Label: 1\n",
      "Input: [2, 8] (['the', 'for']) / Label: 1\n",
      "Input: [2, 99] (['the', 'three']) / Label: 1\n",
      "Input: [99, 2] (['three', 'the']) / Label: 1\n",
      "Input: [99, 199] (['three', 'months']) / Label: 1\n",
      "Input: [199, 99] (['months', 'three']) / Label: 1\n",
      "Input: [199, 3] (['months', 'to']) / Label: 1\n",
      "Input: [3, 199] (['to', 'months']) / Label: 1\n",
      "Input: [3, 338] (['to', 'december']) / Label: 1\n",
      "Input: [338, 3] (['december', 'to']) / Label: 1\n",
      "Input: [338, 1] (['december', '']) / Label: 1\n",
      "Input: [1, 338] (['', 'december']) / Label: 1\n",
      "Input: [3938, 15962] (['quarterly', 'defines']) / Label: 0\n",
      "Input: [3596, 3538] (['warner', 'coverage']) / Label: 0\n",
      "Input: [1, 22001] (['', 'durability']) / Label: 0\n",
      "Input: [5321, 2432] (['76', 'partly']) / Label: 0\n",
      "Input: [3596, 717] (['warner', 'giant']) / Label: 0\n",
      "Input: [626, 13866] (['profits', 'ugandan']) / Label: 0\n",
      "Input: [108, 10510] (['1', 'daggers']) / Label: 0\n",
      "Input: [8, 24511] (['for', 'whimsical']) / Label: 0\n",
      "Input: [717, 29475] (['giant', 'bruises']) / Label: 0\n",
      "Input: [2, 23252] (['the', 'futurology']) / Label: 0\n",
      "Input: [4223, 8406] (['ad', 'krivorizhstal']) / Label: 0\n",
      "Input: [1050, 32010] (['profit', 'bumping']) / Label: 0\n",
      "Input: [8, 20922] (['for', \"'stronger\"]) / Label: 0\n",
      "Input: [187, 9751] (['sales', 'assure']) / Label: 0\n",
      "Input: [1, 12724] (['', 'greening']) / Label: 0\n",
      "Input: [303, 8955] (['media', 'ancic']) / Label: 0\n",
      "Input: [716, 7344] (['boost', 'counterpart']) / Label: 0\n",
      "Input: [3, 6498] (['to', 'eyed']) / Label: 0\n",
      "Input: [8263, 14475] (['timewarner', 'sly']) / Label: 0\n",
      "Input: [303, 17962] (['media', 'abating']) / Label: 0\n",
      "Input: [21, 5290] (['at', \"they'll\"]) / Label: 0\n",
      "Input: [11185, 14845] (['13bn', 'wrecked']) / Label: 0\n",
      "Input: [1050, 7711] (['profit', 'unaffected']) / Label: 0\n",
      "Input: [49, 11866] (['us', 'symmons']) / Label: 0\n",
      "Input: [99, 8124] (['three', \"voters'\"]) / Label: 0\n",
      "Input: [21, 2692] (['at', 'criminals']) / Label: 0\n",
      "Input: [187, 4214] (['sales', 'biarritz']) / Label: 0\n",
      "Input: [716, 13162] (['boost', 'reliant']) / Label: 0\n",
      "Input: [199, 3872] (['months', 'aggressive']) / Label: 0\n",
      "Input: [66, 28795] (['time', \"moments'\"]) / Label: 0\n",
      "Input: [66, 25114] (['time', 'plummeted']) / Label: 0\n",
      "Input: [3, 19934] (['to', \"ask's\"]) / Label: 0\n",
      "Input: [49, 28483] (['us', 'venta']) / Label: 0\n",
      "Input: [108, 1197] (['1', 'offering']) / Label: 0\n",
      "Input: [3938, 21498] (['quarterly', 'sme']) / Label: 0\n",
      "Input: [626, 29198] (['profits', 'ejected']) / Label: 0\n",
      "Input: [338, 27309] (['december', 'sgt']) / Label: 0\n",
      "Input: [717, 18680] (['giant', 'strife']) / Label: 0\n",
      "Input: [1, 24174] (['', 'fraternities']) / Label: 0\n",
      "Input: [11185, 21688] (['13bn', 'vickers']) / Label: 0\n",
      "Input: [3, 17472] (['to', 'winterbottom']) / Label: 0\n",
      "Input: [2972, 19279] (['jumped', \"thomas'\"]) / Label: 0\n",
      "Input: [99, 4836] (['three', 'hart']) / Label: 0\n",
      "Input: [2972, 11235] (['jumped', 'knees']) / Label: 0\n",
      "Input: [199, 12797] (['months', 'sampras']) / Label: 0\n",
      "Input: [8263, 9159] (['timewarner', 'excludes']) / Label: 0\n",
      "Input: [3, 2587] (['to', 'vulnerable']) / Label: 0\n",
      "Input: [5321, 20647] (['76', 'lactating']) / Label: 0\n",
      "Input: [2, 13858] (['the', 'townsend']) / Label: 0\n",
      "Input: [338, 27564] (['december', 'realities']) / Label: 0\n",
      "Input: [3938, 12690] (['quarterly', \"vogts'\"]) / Label: 0\n",
      "Input: [3596, 12505] (['warner', 'repair']) / Label: 0\n",
      "Input: [1, 23346] (['', 'arditti']) / Label: 0\n",
      "Input: [5321, 5811] (['76', 'guitar']) / Label: 0\n",
      "Input: [3596, 32189] (['warner', 'oxby']) / Label: 0\n",
      "Input: [626, 6875] (['profits', 'belfast']) / Label: 0\n",
      "Input: [108, 4779] (['1', 'tapes']) / Label: 0\n",
      "Input: [8, 27853] (['for', 'tardy']) / Label: 0\n",
      "Input: [717, 12772] (['giant', 'enqvist']) / Label: 0\n",
      "Input: [2, 24434] (['the', 'smoked']) / Label: 0\n",
      "Input: [4223, 23787] (['ad', \"l'equipier\"]) / Label: 0\n",
      "Input: [1050, 1878] (['profit', 'evening']) / Label: 0\n",
      "Input: [8, 20891] (['for', '690']) / Label: 0\n",
      "Input: [187, 3997] (['sales', 'morris']) / Label: 0\n",
      "Input: [1, 12073] (['', 'diego']) / Label: 0\n",
      "Input: [303, 30435] (['media', 'persian']) / Label: 0\n",
      "Input: [716, 2925] (['boost', 'predict']) / Label: 0\n",
      "Input: [3, 11757] (['to', 'clarify']) / Label: 0\n",
      "Input: [8263, 29513] (['timewarner', 'enclosure']) / Label: 0\n",
      "Input: [303, 25923] (['media', \"stan's\"]) / Label: 0\n",
      "Input: [21, 7265] (['at', 'regret']) / Label: 0\n",
      "Input: [11185, 21916] (['13bn', '445']) / Label: 0\n",
      "Input: [1050, 31992] (['profit', 'notoriety']) / Label: 0\n",
      "Input: [49, 16937] (['us', 'conservatory']) / Label: 0\n",
      "Input: [99, 6632] (['three', 'slid']) / Label: 0\n",
      "Input: [21, 4018] (['at', 'abused']) / Label: 0\n",
      "Input: [187, 29940] (['sales', 'underrated']) / Label: 0\n",
      "Input: [716, 26337] (['boost', 'auctioneer']) / Label: 0\n",
      "Input: [199, 17414] (['months', \"metallica's\"]) / Label: 0\n",
      "Input: [66, 790] (['time', 'whole']) / Label: 0\n",
      "Input: [66, 13294] (['time', 'implying']) / Label: 0\n",
      "Input: [3, 18888] (['to', 'invitations']) / Label: 0\n",
      "Input: [49, 32288] (['us', 'cybersecurity']) / Label: 0\n",
      "Input: [108, 28893] (['1', 'alcock']) / Label: 0\n",
      "Input: [3938, 1474] (['quarterly', 'secure']) / Label: 0\n",
      "Input: [626, 24642] (['profits', 'cannings']) / Label: 0\n",
      "Input: [338, 27143] (['december', 'guideline']) / Label: 0\n",
      "Input: [717, 21153] (['giant', 'â£352m']) / Label: 0\n",
      "Input: [1, 3343] (['', 'motivated']) / Label: 0\n",
      "Input: [11185, 20763] (['13bn', 'accumulator']) / Label: 0\n",
      "Input: [3, 6629] (['to', 'totalled']) / Label: 0\n",
      "Input: [2972, 11601] (['jumped', 'taxi']) / Label: 0\n",
      "Input: [99, 14698] (['three', 'duke']) / Label: 0\n",
      "Input: [2972, 8084] (['jumped', 'honesty']) / Label: 0\n",
      "Input: [199, 28477] (['months', 'bute']) / Label: 0\n",
      "Input: [8263, 30894] (['timewarner', 'distracts']) / Label: 0\n",
      "Input: [3, 4354] (['to', 'villa']) / Label: 0\n",
      "Input: [5321, 21542] (['76', 'softening']) / Label: 0\n",
      "Input: [2, 28475] (['the', 'asi']) / Label: 0\n",
      "Input: [338, 23542] (['december', 'snobbery']) / Label: 0\n",
      "Input: [3938, 16068] (['quarterly', 'rigs']) / Label: 0\n",
      "Input: [3596, 7716] (['warner', 'graduate']) / Label: 0\n",
      "Input: [1, 25373] (['', 'acne']) / Label: 0\n",
      "Input: [5321, 26032] (['76', \"'silly\"]) / Label: 0\n",
      "Input: [3596, 16178] (['warner', 'â£182']) / Label: 0\n",
      "Input: [626, 29068] (['profits', 'mcintosh']) / Label: 0\n",
      "Input: [108, 22610] (['1', 'cowardly']) / Label: 0\n",
      "Input: [8, 10860] (['for', 'nurse']) / Label: 0\n",
      "Input: [717, 25030] (['giant', 'jamz']) / Label: 0\n",
      "Input: [2, 6654] (['the', '900']) / Label: 0\n",
      "Input: [4223, 23471] (['ad', 'stanislaw']) / Label: 0\n",
      "Input: [1050, 1394] (['profit', 'avoid']) / Label: 0\n",
      "Input: [8, 29594] (['for', 'disallow']) / Label: 0\n",
      "Input: [187, 11880] (['sales', 'lyric']) / Label: 0\n",
      "Input: [1, 3282] (['', 'speaker']) / Label: 0\n",
      "Input: [303, 17499] (['media', 'saunders']) / Label: 0\n",
      "Input: [716, 28485] (['boost', 'ines']) / Label: 0\n",
      "Input: [3, 24677] (['to', 'beachgoer']) / Label: 0\n",
      "Input: [8263, 23496] (['timewarner', 'lange']) / Label: 0\n",
      "Input: [303, 20196] (['media', 'leaped']) / Label: 0\n",
      "Input: [21, 7629] (['at', 'pernod']) / Label: 0\n",
      "Input: [11185, 20344] (['13bn', \"2005's\"]) / Label: 0\n",
      "Input: [1050, 14310] (['profit', 'severn']) / Label: 0\n",
      "Input: [49, 25369] (['us', 'cosmic']) / Label: 0\n",
      "Input: [99, 917] (['three', 'tough']) / Label: 0\n",
      "Input: [21, 12824] (['at', 'shiny']) / Label: 0\n",
      "Input: [187, 2067] (['sales', 'monitor']) / Label: 0\n",
      "Input: [716, 26291] (['boost', 'uprate']) / Label: 0\n",
      "Input: [199, 16240] (['months', 'slowdowns']) / Label: 0\n",
      "Input: [66, 13467] (['time', 'temperature']) / Label: 0\n",
      "Input: [66, 5558] (['time', 'myskina']) / Label: 0\n",
      "Input: [3, 4394] (['to', 'deadline']) / Label: 0\n",
      "Input: [49, 8090] (['us', 'buhecha']) / Label: 0\n",
      "Input: [108, 1819] (['1', '1998']) / Label: 0\n",
      "Input: [3938, 23815] (['quarterly', 'cesars']) / Label: 0\n",
      "Input: [626, 27461] (['profits', 'gchq']) / Label: 0\n",
      "Input: [338, 2112] (['december', 'crisis']) / Label: 0\n",
      "Input: [717, 10748] (['giant', 'benn']) / Label: 0\n",
      "Input: [1, 1227] (['', 'described']) / Label: 0\n",
      "Input: [11185, 18668] (['13bn', '168']) / Label: 0\n",
      "Input: [3, 24940] (['to', 'baffling']) / Label: 0\n",
      "Input: [2972, 24226] (['jumped', 'quad']) / Label: 0\n",
      "Input: [99, 24142] (['three', \"marie's\"]) / Label: 0\n",
      "Input: [2972, 3542] (['jumped', 'holder']) / Label: 0\n",
      "Input: [199, 30466] (['months', 'v603sh']) / Label: 0\n",
      "Input: [8263, 29735] (['timewarner', '1893']) / Label: 0\n",
      "Input: [3, 6396] (['to', 'exceeded']) / Label: 0\n",
      "Input: [5321, 28284] (['76', 'staten']) / Label: 0\n",
      "Input: [2, 1622] (['the', \"isn't\"]) / Label: 0\n",
      "Input: [338, 8780] (['december', 'foo']) / Label: 0\n",
      "Input: [3938, 11730] (['quarterly', 'conceived']) / Label: 0\n",
      "Input: [3596, 11466] (['warner', 'overcrowding']) / Label: 0\n",
      "Input: [1, 3950] (['', 'notice']) / Label: 0\n",
      "Input: [5321, 29742] (['76', 'laughter']) / Label: 0\n",
      "Input: [3596, 26449] (['warner', 'sprees']) / Label: 0\n",
      "Input: [626, 16450] (['profits', 'â£211']) / Label: 0\n",
      "Input: [108, 31921] (['1', 'sphere']) / Label: 0\n",
      "Input: [8, 27937] (['for', \"'sweat\"]) / Label: 0\n",
      "Input: [717, 14219] (['giant', 'hillbillies']) / Label: 0\n",
      "Input: [2, 16076] (['the', \"s's\"]) / Label: 0\n",
      "Input: [4223, 17137] (['ad', 'conquered']) / Label: 0\n",
      "Input: [1050, 27936] (['profit', 'manpower']) / Label: 0\n",
      "Input: [8, 7738] (['for', 'mounting']) / Label: 0\n",
      "Input: [187, 10577] (['sales', 'pirate']) / Label: 0\n",
      "Input: [1, 895] (['', 'investors']) / Label: 0\n",
      "Input: [303, 398] (['media', 'quarter']) / Label: 0\n",
      "Input: [716, 29863] (['boost', 'neumi']) / Label: 0\n",
      "Input: [3, 30536] (['to', 'rechargers']) / Label: 0\n",
      "Input: [8263, 27090] (['timewarner', \"weddings'\"]) / Label: 0\n",
      "Input: [303, 5519] (['media', 'aspect']) / Label: 0\n",
      "Input: [21, 16024] (['at', 'shepherdson']) / Label: 0\n",
      "Input: [11185, 5105] (['13bn', 'consideration']) / Label: 0\n",
      "Input: [1050, 21964] (['profit', 'siphoning']) / Label: 0\n",
      "Input: [49, 2568] (['us', 'jeeves']) / Label: 0\n",
      "Input: [99, 12109] (['three', 'cop']) / Label: 0\n",
      "Input: [21, 16548] (['at', 'jumpy']) / Label: 0\n",
      "Input: [187, 30514] (['sales', \"contacts'\"]) / Label: 0\n",
      "Input: [716, 21363] (['boost', 'kazuhiro']) / Label: 0\n",
      "Input: [199, 19474] (['months', 'sivright']) / Label: 0\n",
      "Input: [66, 7758] (['time', 'restricting']) / Label: 0\n",
      "Input: [66, 31996] (['time', 'leakers']) / Label: 0\n",
      "Input: [3, 14415] (['to', 'adarsh']) / Label: 0\n",
      "Input: [49, 11825] (['us', 'comparable']) / Label: 0\n",
      "Input: [108, 26597] (['1', \"'denial'\"]) / Label: 0\n",
      "Input: [3938, 14516] (['quarterly', 'eyebrows']) / Label: 0\n",
      "Input: [626, 19060] (['profits', 'coolly']) / Label: 0\n",
      "Input: [338, 15855] (['december', 'renegotiation']) / Label: 0\n",
      "Input: [717, 28028] (['giant', 'parchment']) / Label: 0\n",
      "Input: [1, 26164] (['', \"'menace'\"]) / Label: 0\n",
      "Input: [11185, 24911] (['13bn', 'baked']) / Label: 0\n",
      "Input: [3, 15586] (['to', 'havana']) / Label: 0\n",
      "Input: [2972, 17078] (['jumped', 'engrossing']) / Label: 0\n",
      "Input: [99, 31874] (['three', 'rohm']) / Label: 0\n",
      "Input: [2972, 24364] (['jumped', 'emmer']) / Label: 0\n",
      "Input: [199, 22726] (['months', '086']) / Label: 0\n",
      "Input: [8263, 22005] (['timewarner', 'tadeu']) / Label: 0\n",
      "Input: [3, 8399] (['to', 'contributing']) / Label: 0\n",
      "Input: [5321, 22306] (['76', 'tetrahydrocannabinol']) / Label: 0\n",
      "Input: [2, 1892] (['the', 'cap']) / Label: 0\n",
      "Input: [338, 8202] (['december', 'laharrague']) / Label: 0\n"
     ]
    }
   ],
   "source": [
    "window_size = 1 # How many words to consider left and right.\n",
    "\n",
    "inputs, labels = tf.keras.preprocessing.sequence.skipgrams(\n",
    "    tokenizer.texts_to_sequences([news_stories[0][:150]])[0],\n",
    "    vocabulary_size=len(tokenizer.word_index.items())+1,\n",
    "    window_size=window_size, negative_samples=4, shuffle=False,\n",
    "    categorical=False, sampling_table=None, seed=None\n",
    ")\n",
    "\n",
    "i = 0\n",
    "for inp, lbl in zip(inputs, labels):\n",
    "    i += 1\n",
    "    print(f\"Input: {inp} ({[tokenizer.index_word[wi] for wi in inp]}) / Label: {lbl}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f339a44a-55e4-487f-b137-7c7255af4121",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4096 # Data points in a single batch\n",
    "\n",
    "embedding_size = 128 # Dimension of the embedding vector.\n",
    "\n",
    "window_size=1 # We use a window size of 1 on either side of target word\n",
    "epochs = 5 # Number of epochs to train for\n",
    "negative_samples = 4 # Number of negative samples generated per example\n",
    "\n",
    "# We pick a random validation set to sample nearest neighbors\n",
    "valid_size = 16 # Random set of words to evaluate similarity on.\n",
    "# We sample valid data points randomly from a large window without always being deterministic\n",
    "valid_window = 250\n",
    "\n",
    "# When selecting valid examples, we select some of the most frequent words as well as\n",
    "# some moderately rare words\n",
    "np.random.seed(54321)\n",
    "random.seed(54321)\n",
    "\n",
    "valid_term_ids = np.array(random.sample(range(valid_window), valid_size))\n",
    "valid_term_ids = np.append(\n",
    "    valid_term_ids, random.sample(range(1000, 1000+valid_window), valid_size),\n",
    "    axis=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5e027bcc-d0a1-486c-9790-5c7ea3898cce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"cbow_model\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"cbow_model\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ context_embedding   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,920,128</span> │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)            │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ context_embeddin… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ target_embedding    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,920,128</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dot (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dot</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lambda[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     │\n",
       "│                     │                   │            │ target_embedding… │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ context_embedding   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │  \u001b[38;5;34m1,920,128\u001b[0m │ input_layer_1[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer         │ (\u001b[38;5;45mNone\u001b[0m)            │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda (\u001b[38;5;33mLambda\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ context_embeddin… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ target_embedding    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │  \u001b[38;5;34m1,920,128\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dot (\u001b[38;5;33mDot\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ lambda[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     │\n",
       "│                     │                   │            │ target_embedding… │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,840,256</span> (14.65 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,840,256\u001b[0m (14.65 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,840,256</span> (14.65 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,840,256\u001b[0m (14.65 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "\n",
    "# Inputs; target input layer will have the final shape [None]\n",
    "# context will have [None, 2xwindow_size] shape\n",
    "input_1 = tf.keras.layers.Input(shape=())\n",
    "\n",
    "input_2 = tf.keras.layers.Input(shape=(window_size*2,))\n",
    "\n",
    "# Target and context embedding layers\n",
    "target_embedding_layer = tf.keras.layers.Embedding(\n",
    "    input_dim=n_vocab, output_dim=embedding_size, name='target_embedding'\n",
    ")\n",
    "\n",
    "context_embedding_layer = tf.keras.layers.Embedding(\n",
    "    input_dim=n_vocab, output_dim=embedding_size, name='context_embedding'\n",
    ")\n",
    "\n",
    "# Outputs of the target and context embedding lookups\n",
    "context_out = context_embedding_layer(input_2)\n",
    "target_out = target_embedding_layer(input_1)\n",
    "\n",
    "# Taking the mean over the all the context words to produce [None, embedding_size]\n",
    "mean_context_out = tf.keras.layers.Lambda(lambda x: tf.reduce_mean(x, axis=1))(context_out)\n",
    "\n",
    "# Computing the dot product between the two\n",
    "out = tf.keras.layers.Dot(axes=-1)([mean_context_out, target_out])\n",
    "\n",
    "cbow_model = tf.keras.models.Model(inputs=[input_1, input_2], outputs=out, name='cbow_model')\n",
    "\n",
    "cbow_model.compile(\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "    optimizer='adam'\n",
    ")\n",
    "\n",
    "cbow_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "72831649-fff9-4120-ae37-7d2c58adac50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValidationCallback(tf.keras.callbacks.Callback):\n",
    "\n",
    "    def __init__(self, valid_term_ids, model_with_embeddings, tokenizer):\n",
    "\n",
    "        self.valid_term_ids = valid_term_ids\n",
    "        self.model_with_embeddings = model_with_embeddings\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        \"\"\" Validation logic \"\"\"\n",
    "\n",
    "        # We will use context embeddings to get the most similar words\n",
    "        # Other strategies include: using target embeddings, mean embeddings after avaraging context/target\n",
    "        embedding_weights = self.model_with_embeddings.get_layer(\"context_embedding\").get_weights()[0]\n",
    "        normalized_embeddings = embedding_weights / np.sqrt(np.sum(embedding_weights**2, axis=1, keepdims=True))\n",
    "\n",
    "        # Get the embeddings corresponding to valid_term_ids\n",
    "        valid_embeddings = normalized_embeddings[self.valid_term_ids, :]\n",
    "\n",
    "        # Compute the similarity between valid_term_ids and all the embeddings\n",
    "        # V x d (d x D) => V x D\n",
    "        top_k = 5 # Top k items will be displayed\n",
    "        similarity = np.dot(valid_embeddings, normalized_embeddings.T)\n",
    "\n",
    "        # Invert similarity matrix to negative\n",
    "        # Ignore the first one because that would be the same word as the probe word\n",
    "        similarity_top_k = np.argsort(-similarity, axis=1)[:, 1: top_k+1]\n",
    "\n",
    "        # Print the output\n",
    "        for i, term_id in enumerate(valid_term_ids):\n",
    "\n",
    "            similar_word_str = ', '.join([self.tokenizer.index_word[j] for j in similarity_top_k[i, :] if j >= 1])\n",
    "            print(f\"{self.tokenizer.index_word[term_id]}: {similar_word_str}\")\n",
    "\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9f53338b-1a71-4cdc-b860-4d587f7e5425",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cbow_data_generator(sequences, window_size, batch_size, negative_samples):\n",
    "\n",
    "    rand_sequence_ids = np.arange(len(sequences))\n",
    "    np.random.shuffle(rand_sequence_ids)\n",
    "\n",
    "    for si in rand_sequence_ids:\n",
    "        inputs, labels = cbow_grams(\n",
    "            sequences[si],\n",
    "            vocabulary_size=n_vocab,\n",
    "            window_size=window_size,\n",
    "            negative_samples=negative_samples,\n",
    "            shuffle=True,\n",
    "            sampling_table=sampling_table,\n",
    "            seed=None\n",
    "        )\n",
    "\n",
    "        inputs_context, inputs_target, labels = np.array([inp[1] for inp in inputs]), np.array([inp[0] for inp in inputs]), np.array(labels).reshape(-1,1)\n",
    "\n",
    "        assert inputs_context.shape[0] == inputs_target.shape[0]\n",
    "        assert inputs_context.shape[0] == labels.shape[0]\n",
    "\n",
    "        #print(inputs_context.shape, inputs_target.shape, labels.shape)\n",
    "        for eg_id_start in range(0, inputs_context.shape[0], batch_size):\n",
    "\n",
    "            yield (\n",
    "                inputs_target[eg_id_start: min(eg_id_start+batch_size, inputs_target.shape[0])],\n",
    "                inputs_context[eg_id_start: min(eg_id_start+batch_size, inputs_context.shape[0]),:]\n",
    "            ), labels[eg_id_start: min(eg_id_start+batch_size, labels.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0b675a38-5437-4257-9ae1-ec8dc24668dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/5 started\n",
      "   2226/Unknown \u001b[1m47s\u001b[0m 21ms/step - loss: 0.4978election: lee, rubber, titled, argentine, keith\n",
      "me: around, whether, airport, websites, virtual\n",
      "with: but, during, absolutely, these, some\n",
      "you: they, not, we, help, could\n",
      "were: or, until, v, ross, these\n",
      "win: china, michalak, arsenal's, takes, before\n",
      "those: v, before, around, i'm, lions\n",
      "music: china's, some, illegal, uk, despite\n",
      "also: spokesman, weren't, how, mcgeady, worked\n",
      "third: fourth, 40, first, russian, most\n",
      "best: irish, idol, csi, including, scotland's\n",
      "him: fish, beat, outside, must, v\n",
      "too: neither, goalkeeper, paid, explains, v\n",
      "some: our, reported, greek, my, illegal\n",
      "through: v, pretty, criminal, greater, clocked\n",
      "mr: tony, michael, who, ron, said\n",
      "file: whose, presenter, v, euros, include\n",
      "pair: china, presenter, candidate, whose, islamic\n",
      "ceremony: whose, turns, candidate, plasma, chairman\n",
      "believed: be, doing, sinking, chorus, infection\n",
      "post: singer, infected, rapper, spam, men\n",
      "indian: euros, became, receive, quoted, including\n",
      "successful: seed, chairman, equal, â£3, theatre\n",
      "care: deeply, spread, windsor, andrew, chairman\n",
      "russia: together, via, jet, computing, default\n",
      "talk: bedingfield, controversial, islamic, jackson, whose\n",
      "programs: cyprus, printed, thai, j, candidate\n",
      "fair: side, original, family, thailand, cech\n",
      "hollywood: yen, criminal, singer, india's, controversial\n",
      "attempt: model, striker, starring, presenter, chairman\n",
      "leave: reviews, dj, shows, disease, guy\n",
      "light: awards, fake, welcomed, britain's, china's\n",
      "\n",
      "\n",
      "\u001b[1m2226/2226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 21ms/step - loss: 0.4978\n",
      "Epoch: 2/5 started\n",
      "      4/Unknown \u001b[1m0s\u001b[0m 18ms/step - loss: 0.3989"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ariji\\anaconda3\\Lib\\contextlib.py:158: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   2226/Unknown \u001b[1m48s\u001b[0m 22ms/step - loss: 0.3888election: motors, attorney, spanish, land, cities\n",
      "me: them, didn't, whether, him, tuned\n",
      "with: reacting, tough, bringing, declaring, voice\n",
      "you: they, we, not, help, don't\n",
      "were: are, being, was, get, if\n",
      "win: delivered, yen, â£4, 4m, gold\n",
      "those: ellen, antonio, arrival, dates, gael\n",
      "music: business, â£48, bond, north, china's\n",
      "also: never, already, quite, been, thought\n",
      "third: fourth, first, final, fifth, total\n",
      "best: category, festival, expectations, supporting, musical\n",
      "him: them, doesn't, himself, successfully, whether\n",
      "too: pretty, dedicated, extremely, pig, fresh\n",
      "some: marrying, tim's, taxi, gameplay, scots\n",
      "through: specially, renner, henry's, successfully, publicly\n",
      "mr: tony, bernie, michael, hughes, gordon\n",
      "file: illegally, hopes, carmaker, presenter, older\n",
      "pair: presenter, subs, fashion, afi, turns\n",
      "ceremony: turns, winner, whose, monsanto, barroso\n",
      "believed: stallone, dominic, interviewed, farrell's, featured\n",
      "post: athlete, ingram, perry, case, nuclear\n",
      "indian: cloud, crawford, tourist, prosecutor, slimmer\n",
      "successful: welcomed, monsanto, thai, epic, andersen\n",
      "care: special, dramatic, increases, ufj, virtual\n",
      "russia: outfit, cat, dating, alfred, rinaldi\n",
      "talk: flanker, jackson, steve, eddie, 4m\n",
      "programs: accurate, orlando, river, artists, rail\n",
      "fair: von, tackling, usher's, committing, kewell\n",
      "hollywood: yen, controversial, sang, concedes, head\n",
      "attempt: ice, celtic, veteran, starring, boy\n",
      "leave: visited, 8m, danish, reviews, defeated\n",
      "light: masi, artists, persico, christine, gangs\n",
      "\n",
      "\n",
      "\u001b[1m2226/2226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 22ms/step - loss: 0.3888\n",
      "Epoch: 3/5 started\n",
      "   2224/Unknown \u001b[1m49s\u001b[0m 22ms/step - loss: 0.3566election: attorney, motors, cities, idol, euro\n",
      "me: them, something, how, him, anyone\n",
      "with: bringing, strained, between, reacting, together\n",
      "you: they, we, don't, i, really\n",
      "were: are, need, have, because, happens\n",
      "win: festival, gold, attend, correspondent, athlete\n",
      "those: wedding, ever, gael, continually, sec\n",
      "music: uk, rivalry, shooting, north, china's\n",
      "also: never, already, quite, always, finally\n",
      "third: fourth, first, second, final, fifth\n",
      "best: festival, idol, award, supporting, category\n",
      "him: anyone, them, doesn't, successfully, might\n",
      "too: extremely, very, pretty, bit, staggering\n",
      "some: taxi, sustainability, scots, gameplay, idn\n",
      "through: surging, specially, arrangements, sends, fate\n",
      "mr: tony, gordon, bernie, hughes, jack\n",
      "file: illegally, charly, presenter, debutant, hopes\n",
      "pair: sentiments, subs, resorts, commit, costed\n",
      "ceremony: men's, opposition, moore's, successful, exel\n",
      "believed: chose, brian, referee, denied, believes\n",
      "post: eastenders', gloom, jones', corp's, featuring\n",
      "indian: titan, worker, tourist, luxury, including\n",
      "successful: exel, mydoom, seeded, restaurant, thai\n",
      "care: policing, onboard, won't, distribute, virtual\n",
      "russia: questioning, cat, resentment, tired, watson\n",
      "talk: jackson, bell, singer, nominee, jones\n",
      "programs: artists, accurate, jibe, us's, turnover\n",
      "fair: norwegian, von, bombing, robotic, crushing\n",
      "hollywood: yen, sang, luxury, eddie, leslie\n",
      "attempt: debut, birmingham, 70, partner, boy\n",
      "leave: elliott, ran, pay, put, 35bn\n",
      "light: artists, adds, injuries, britain's, markedly\n",
      "\n",
      "\n",
      "\u001b[1m2226/2226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 22ms/step - loss: 0.3566\n",
      "Epoch: 4/5 started\n",
      "   2226/Unknown \u001b[1m51s\u001b[0m 23ms/step - loss: 0.3253election: attorney, motors, elections, november's, conservatives'\n",
      "me: anyone, them, automatically, something, how\n",
      "with: reopen, relaxation, locking, solved, o'neill\n",
      "you: we, they, i, actually, don't\n",
      "were: are, have, because, need, get\n",
      "win: â£2, desailly, festival, gold, bronze\n",
      "those: fans', wedding, timed, coveted, utterly\n",
      "music: terrestrial, cameras, legitimate, television's, bond\n",
      "also: never, already, previously, quite, always\n",
      "third: fourth, first, second, final, fifth\n",
      "best: supporting, idol, artist, festival, counterparts\n",
      "him: them, me, doesn't, anyone, politicians\n",
      "too: very, extremely, pretty, so, clearly\n",
      "some: idn, colourful, sad, cinematic, fao\n",
      "through: surging, sends, specially, fate, may\n",
      "mr: tony, gordon, bernie, viktor, hughes\n",
      "file: illegally, charly, debutant, begun, faces\n",
      "pair: costed, wheelchair, sentiments, 97, promptly\n",
      "ceremony: exel, monsanto, winner, composed, debates\n",
      "believed: curled, umpire, opposing, prosecute, rees\n",
      "post: telekom, boerse's, film's, ingram, entire\n",
      "indian: ryanair, titan, worker, tourist, grabbed\n",
      "successful: brewer, seeded, worlds, jerry, competitor\n",
      "care: opinions, virtual, ride, policing, won't\n",
      "russia: invasive, weapon, occasionally, skies, heroine\n",
      "talk: jackson, flanker, bell, jones, hutt\n",
      "programs: accurate, wednesday's, administrative, mydoom, legendary\n",
      "fair: von, chorus, prominently, athletic, kewell\n",
      "hollywood: veteran, campaigner, hopes, miami, hamilton\n",
      "attempt: jones, grimes, partner, elton, birmingham\n",
      "leave: pay, ended, ran, 35bn, winger\n",
      "light: injuries, nanny, artists, beckham, persico\n",
      "\n",
      "\n",
      "\u001b[1m2226/2226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 23ms/step - loss: 0.3253\n",
      "Epoch: 5/5 started\n",
      "   2226/Unknown \u001b[1m48s\u001b[0m 21ms/step - loss: 0.3031election: attorney, motors, november's, batter, workers'\n",
      "me: them, him, anyone, automatically, you're\n",
      "with: reopen, walking, blend, closely, locking\n",
      "you: we, they, i, i'll, don't\n",
      "were: are, have, need, want, there's\n",
      "win: â£2, visit, evil, â£11, 53\n",
      "those: affluent, others, appeals, pcs, considering\n",
      "music: cameras, divide, legitimate, terrestrial, company's\n",
      "also: already, previously, never, quite, always\n",
      "third: fourth, second, first, fifth, final\n",
      "best: idol, category, irish, artist, screenplay\n",
      "him: them, me, anyone, politicians, rovers\n",
      "too: very, extremely, so, clearer, pretty\n",
      "some: bits, taxi, preliminary, user's, gameplay\n",
      "through: sends, surging, specially, we'd, offshore\n",
      "mr: tony, gordon, viktor, jack, bernie\n",
      "file: illegally, charly, begun, faces, tiger\n",
      "pair: wheelchair, thailand, costed, springboks, financially\n",
      "ceremony: brit, guild, academy, counterparts, exel\n",
      "believed: denied, happen, delighted, defended, umpire\n",
      "post: volkswagen, london's, boerse's, mydoom, gloom\n",
      "indian: cities, ryanair, listing, tourist, korean\n",
      "successful: artist, creating, banking, brewer, joystick\n",
      "care: virtual, policing, behaviour, mountain, improvements\n",
      "russia: mines, resurgence, weapon, interpretation, occasionally\n",
      "talk: jackson, cole, bell, gareth, hague\n",
      "programs: mydoom, program, microsoft's, draft, brazil's\n",
      "fair: contact, negotiate, von, robotic, condom\n",
      "hollywood: sang, veteran, 1980, washed, nominee\n",
      "attempt: partner, 73, cook, bez, controller\n",
      "leave: pay, plus, henson, ran, measure\n",
      "light: injuries, beckham, brands, nanny, mistrust\n",
      "\n",
      "\n",
      "\u001b[1m2226/2226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 21ms/step - loss: 0.3031\n"
     ]
    }
   ],
   "source": [
    "cbow_validation_callback = ValidationCallback(valid_term_ids, cbow_model, tokenizer)\n",
    "\n",
    "for ei in range(epochs):\n",
    "    print(f\"Epoch: {ei+1}/{epochs} started\")\n",
    "    news_cbow_gen = cbow_data_generator(news_sequences, window_size, batch_size, negative_samples)\n",
    "    cbow_model.fit(\n",
    "        news_cbow_gen,\n",
    "        epochs=1,\n",
    "        callbacks=cbow_validation_callback,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "31fa0e98-e4db-4673-947c-647b411b9219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "[ 0.11085108 -0.25469235 -0.15203784  0.07325227 -0.14737785  0.14830768\n",
      " -0.27429166 -0.09608977  0.03704134 -0.17361996  0.01531993 -0.0465077\n",
      " -0.23753951 -0.25014573 -0.0253557   0.40826246 -0.16949935  0.01692696\n",
      " -0.20239356  0.14066818  0.22502121  0.14511508  0.21380092 -0.32108378\n",
      "  0.15176813  0.13242695 -0.2073735   0.0660826  -0.14078793  0.30938998\n",
      "  0.2563888  -0.11537783 -0.057417    0.16718397 -0.06057169  0.12312715\n",
      " -0.26447493  0.14412111 -0.24513826  0.20877922 -0.10309254  0.26646736\n",
      "  0.1685377   0.12303187 -0.1645519   0.09164843  0.19170655 -0.13821886\n",
      "  0.21663491  0.05457383 -0.31042415 -0.17717057 -0.12289714  0.20443682\n",
      "  0.23057601  0.0138363  -0.11138823 -0.2888349  -0.28080335  0.16014308\n",
      " -0.20665485  0.09823053 -0.25267482  0.11103029  0.10458331 -0.20751747\n",
      " -0.15295331  0.02635166 -0.15592682 -0.31063843  0.14527409  0.37395683\n",
      "  0.28519127 -0.04358221  0.0892965  -0.20123228 -0.25392875  0.251636\n",
      " -0.0364037   0.05037522 -0.22806315 -0.4000495   0.08008419  0.09942783\n",
      " -0.33077133 -0.0685747  -0.17021601  0.18103576 -0.2365621   0.08771051\n",
      " -0.12803003  0.05744473 -0.27765468 -0.13037613  0.13081877  0.02993144\n",
      "  0.10983201 -0.23823395  0.30642235  0.15889978  0.17122558  0.14596507\n",
      "  0.24100834  0.2700518   0.1562949   0.07057667  0.35843664  0.07942131\n",
      " -0.3196367   0.18700404  0.04059797  0.06500164  0.10299201  0.4190156\n",
      " -0.13926588 -0.20865229 -0.07493755  0.18548483  0.13947307 -0.08150068\n",
      " -0.27324843  0.2679969  -0.27275905  0.18993163  0.21308224 -0.10987978\n",
      "  0.09643088  0.04574496]\n"
     ]
    }
   ],
   "source": [
    "# example 1 : word vector for \"dog\"\n",
    "word_vector_dog = cbow_model.get_layer(\"context_embedding\").get_weights()[0][tokenizer.word_index[\"dog\"]]\n",
    "\n",
    "print(len(word_vector_dog))\n",
    "print(word_vector_dog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a7578064-6f0e-442d-980c-278330b9acc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "[ 0.127196   -0.11693152  0.09308042  0.12874417 -0.2682941   0.2792257\n",
      " -0.16069162 -0.18182817  0.14409874 -0.18055683  0.18285744 -0.15229836\n",
      " -0.16488212 -0.02559988  0.05636497  0.18549453 -0.18495917  0.14344405\n",
      " -0.2388128   0.1661385   0.02947032  0.2424594   0.2088696  -0.06145832\n",
      "  0.21484898  0.20510563 -0.27681673  0.14117916 -0.18595581  0.20219317\n",
      "  0.175478   -0.11449622 -0.23579505  0.16107257 -0.09269111  0.19590852\n",
      " -0.25171912  0.10727876 -0.2352388   0.16605994 -0.12294747  0.1159846\n",
      "  0.24661736  0.32330567 -0.25087246  0.24554014  0.11941528 -0.31488204\n",
      "  0.00839808 -0.21290123 -0.03791186 -0.16514829 -0.14536686  0.08121478\n",
      "  0.18461722  0.1893544  -0.15681821 -0.2482141  -0.1162774   0.19971709\n",
      " -0.05162466  0.30691355 -0.10760359  0.23771767  0.23367436  0.15234634\n",
      " -0.1659805  -0.27187628 -0.14614631 -0.11600802  0.0781921  -0.10022354\n",
      "  0.13923761  0.17586756  0.26802883 -0.21206842 -0.26949224 -0.01797035\n",
      " -0.15069278 -0.00377987 -0.23673578 -0.14481755  0.39280882 -0.02184564\n",
      " -0.21027625  0.10314216 -0.31069544  0.22059926 -0.11323918  0.17385203\n",
      " -0.266603    0.1615024  -0.18910295 -0.08890793  0.15388139  0.09497244\n",
      "  0.12324887 -0.34354934  0.06236477  0.01473716  0.20655443  0.2936853\n",
      "  0.12417828  0.13107136  0.24495809  0.2435061   0.09446736  0.06376769\n",
      " -0.12021033  0.2182826  -0.07509553  0.18473183  0.18518358  0.18687636\n",
      " -0.21552116 -0.12704742 -0.15548408 -0.01060209  0.0552235  -0.15897587\n",
      " -0.30199796  0.07940581 -0.11245829  0.16451807  0.27578163 -0.1702784\n",
      " -0.16405998  0.1642449 ]\n"
     ]
    }
   ],
   "source": [
    "# example 2 : word vector for \"cat\"\n",
    "word_vector_cat = cbow_model.get_layer(\"context_embedding\").get_weights()[0][tokenizer.word_index[\"cat\"]]\n",
    "\n",
    "print(len(word_vector_cat))\n",
    "print(word_vector_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "38672179-1dde-4ffe-a547-b01001197f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "[ 6.86640143e-02 -5.90903699e-01  3.10078174e-01  1.72983125e-01\n",
      "  2.41807267e-01  5.43345094e-01  2.31243357e-01 -2.29310557e-01\n",
      " -1.76859405e-02 -1.75965786e-01  3.34119797e-01  5.26852794e-02\n",
      "  7.04236478e-02 -2.15318263e-01  2.41668685e-03  2.40028322e-01\n",
      " -3.42839777e-01  4.21943843e-01 -1.38924852e-01 -2.80066907e-01\n",
      " -1.63855374e-01  4.69146715e-03 -8.69998187e-02 -6.29312634e-01\n",
      "  3.13283026e-01 -1.95082814e-01 -3.25315177e-01 -1.87254459e-01\n",
      " -1.81876370e-04  5.31756580e-01 -2.61715055e-02  2.41853774e-01\n",
      " -4.42755371e-01 -5.55196293e-02 -9.73677486e-02 -1.19620278e-01\n",
      " -4.69525784e-01 -8.47011358e-02 -5.05936861e-01  3.19596171e-01\n",
      " -1.28067613e-01  4.31536585e-01  3.07860300e-02 -5.19085899e-02\n",
      " -5.43045290e-02 -1.22711517e-01  1.36814132e-01  5.43368421e-02\n",
      " -5.22495471e-02 -1.80196136e-01  5.21681488e-01 -2.45501578e-01\n",
      " -1.76035956e-01  2.38810629e-01 -1.66951284e-01 -1.26013026e-01\n",
      " -1.09804198e-01 -3.73923123e-01 -1.64061233e-01  3.14823955e-01\n",
      " -1.07101746e-01 -2.60063291e-01 -1.77954193e-02  4.89395373e-02\n",
      "  1.59952313e-01 -1.30483434e-01 -8.92232731e-02 -2.92499602e-01\n",
      " -2.19827101e-01 -1.21752784e-01  2.15733334e-01  3.77011210e-01\n",
      "  4.62532312e-01  4.18572910e-02  3.41341287e-01 -2.70014167e-01\n",
      "  1.21461332e-01 -5.47741532e-01  6.03061393e-02 -4.19759154e-01\n",
      "  6.80500343e-02 -1.88705087e-01  4.00885165e-01  1.02628008e-01\n",
      " -4.20775190e-02  3.68855819e-02 -2.51314372e-01 -4.38987687e-02\n",
      " -1.47239670e-01 -2.05084950e-01 -2.91995138e-01 -5.19307256e-02\n",
      " -7.00297773e-01 -2.68367290e-01  2.73219764e-01  1.87695220e-01\n",
      " -2.11685523e-01  8.46900865e-02  5.14913857e-01  6.85856789e-02\n",
      "  3.06140874e-02  1.95655584e-01  4.14127976e-01  3.05723578e-01\n",
      "  2.96681404e-01  7.44576082e-02 -1.24504715e-02  7.47110918e-02\n",
      " -1.43278837e-01  3.14239040e-02  1.00416556e-01 -2.23238140e-01\n",
      "  4.42552604e-02  5.67257628e-02 -3.55019778e-01 -1.92599967e-01\n",
      " -1.04339033e-01 -5.20735443e-01  8.92410636e-01 -1.26590416e-01\n",
      " -7.46434778e-02  8.46798271e-02  3.33142243e-02  1.88983321e-01\n",
      "  2.66863734e-01 -8.41210932e-02 -7.07203941e-03 -1.76001742e-01]\n"
     ]
    }
   ],
   "source": [
    "# example 3 : word vector for \"man\"\n",
    "word_vector_man = cbow_model.get_layer(\"context_embedding\").get_weights()[0][tokenizer.word_index[\"man\"]]\n",
    "\n",
    "print(len(word_vector_man))\n",
    "print(word_vector_man)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "58e43419-def7-42b0-8eca-cf8b211c60be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "[ 0.39148128 -0.34370282  0.19532591  0.06584781 -0.15752243  0.44233474\n",
      "  0.01061332 -0.07237288  0.21496254 -0.02258785  0.07142099 -0.0707683\n",
      " -0.58668065 -0.23823674  0.06922557  0.00662435 -0.07818599 -0.15131135\n",
      "  0.01688558  0.02565692  0.03561234 -0.00243727  0.04619027 -0.3215033\n",
      "  0.09538077  0.074044   -0.13624005 -0.13199133 -0.13095568  0.4468145\n",
      "  0.07778834 -0.02715021 -0.25235435 -0.11935147 -0.26052564  0.02816335\n",
      "  0.02029142  0.2005923   0.09127384  0.0535125   0.02829764  0.33092332\n",
      "  0.32427287  0.09690747 -0.00766895  0.24605803 -0.05985556 -0.10512602\n",
      "  0.0754553  -0.19038695  0.1628446  -0.26858997 -0.3392183   0.13743076\n",
      "  0.14457273  0.32185763 -0.08765571 -0.30394438 -0.25770885  0.10989341\n",
      " -0.11284456 -0.01070174 -0.10117409 -0.01480832  0.27981734  0.09945339\n",
      " -0.2003053   0.01811188 -0.52966523 -0.16829725 -0.15140848  0.09773593\n",
      "  0.30029017 -0.2774232   0.00939985 -0.12097496 -0.07648011  0.5156046\n",
      "  0.06540091 -0.36145192  0.05068285  0.01117715  0.03669181 -0.12186175\n",
      " -0.00315948  0.16006847 -0.14099799  0.03183717 -0.32264403 -0.03106774\n",
      " -0.22560556  0.15935908 -0.2649109  -0.13685308  0.18169922  0.03150124\n",
      "  0.0807114  -0.36229405  0.43414915  0.17237096  0.14679411  0.17968988\n",
      "  0.2750406   0.18533497  0.14274836  0.00888175  0.0779627   0.17409068\n",
      " -0.16230378  0.02025802 -0.09252357  0.03385055  0.10881934  0.13441475\n",
      " -0.00090756 -0.1750164  -0.12075703 -0.4996769   0.44557533 -0.00321493\n",
      " -0.19385894  0.1240289   0.05102829 -0.0016187  -0.10536381 -0.09185158\n",
      "  0.09995649  0.3058309 ]\n"
     ]
    }
   ],
   "source": [
    "# example 4 : word vector for \"woman\"\n",
    "word_vector_woman = cbow_model.get_layer(\"context_embedding\").get_weights()[0][tokenizer.word_index[\"woman\"]]\n",
    "\n",
    "print(len(word_vector_woman))\n",
    "print(word_vector_woman)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6c8a88-22f0-4be3-a0da-64eebd9df5ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8d74b2d0-e32d-457c-8ac8-d30b26db667c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7114462\n"
     ]
    }
   ],
   "source": [
    "# example 1 : similarity score between dog and cat\n",
    "similarity = np.dot(word_vector_dog, word_vector_cat) / (np.linalg.norm(word_vector_dog) * np.linalg.norm(word_vector_cat))\n",
    "\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "fb8dba24-1bfa-4499-bc76-3d927b0dfe62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4212371\n"
     ]
    }
   ],
   "source": [
    "# example 2 : similarity score between dog and man\n",
    "similarity = np.dot(word_vector_dog, word_vector_man) / (np.linalg.norm(word_vector_dog) * np.linalg.norm(word_vector_man))\n",
    "\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b3fe9064-e729-49c8-8f02-37308e7f5f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4896157\n"
     ]
    }
   ],
   "source": [
    "# example 3 : similarity score between dog and man\n",
    "similarity = np.dot(word_vector_man, word_vector_woman) / (np.linalg.norm(word_vector_man) * np.linalg.norm(word_vector_woman))\n",
    "\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48e6a9d-e96c-4e82-9d3f-f831138d55ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f7aa073e-9b7f-44eb-a84e-acc3110d8aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "king_vector = cbow_model.get_layer(\"context_embedding\").get_weights()[0][tokenizer.word_index[\"king\"]]\n",
    "man_vector = cbow_model.get_layer(\"context_embedding\").get_weights()[0][tokenizer.word_index[\"man\"]]\n",
    "woman_vector = cbow_model.get_layer(\"context_embedding\").get_weights()[0][tokenizer.word_index[\"woman\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "48ce470f-b23a-4a6c-9b03-7cbce4fcedae",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Queen'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[72], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mword_index[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQueen\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Queen'"
     ]
    }
   ],
   "source": [
    "tokenizer.word_index[\"Queen\"] ## doesnn't exist in the word index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "77844305-134c-48eb-afef-5357d6c291ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "[ 0.7886466  -0.3211837  -0.30918813 -0.128811   -0.45703763  0.5096866\n",
      " -0.25101283  0.04727484  0.20742783  0.33350104 -0.27290428 -0.0850784\n",
      " -1.0004001  -0.39230192  0.12155542  0.05303115  0.20978881  0.21571879\n",
      " -0.16429172  0.7593962   0.34604892  0.02282587  0.18897085  0.2065222\n",
      " -0.25181735  0.27411595 -0.38478714  0.0096622  -0.3301951   0.23001012\n",
      "  0.2630404  -0.17466757 -0.3652876   0.03548632 -0.20340088  0.23894495\n",
      "  0.21910855  0.4870485  -0.00606919 -0.23775662  0.27549258  0.39024654\n",
      "  0.36853722  0.30672196  0.21343686  0.5633356   0.05765507 -0.2453014\n",
      "  0.0044115  -0.8759689  -0.6061471  -0.632746   -0.49215776  0.3611431\n",
      "  0.32715482  0.5493635  -0.28762263  0.02026632 -0.7933093  -0.07088664\n",
      "  0.05956777  0.4000916  -0.28030956  0.08640276  0.21596453  0.27023456\n",
      " -0.21178803  0.13050488 -0.6706782  -0.6504279   0.22607262 -0.33090222\n",
      "  0.45109263 -0.3017638  -0.2966171   0.35299832 -0.03177927  1.5211345\n",
      " -0.04413279 -0.21507785 -0.10389377 -0.02145489 -0.310586   -0.06976034\n",
      "  0.1886156   0.20792061 -0.14779718 -0.06643613 -0.49347138  0.47632533\n",
      " -0.22060813  0.08008297  0.06788528 -0.08650745 -0.02892573 -0.22361407\n",
      "  0.25142774 -0.5263051   0.29810563  0.24943417  0.1013223   0.15246284\n",
      "  0.9050404   0.16394515  0.3445835  -0.16141897  0.30689308 -0.00728017\n",
      " -0.07570334  0.76294327 -0.34373587  0.34496462  0.02334855  0.31379783\n",
      "  0.03020214 -0.7373546   0.04345955 -0.26814243  0.1604313   0.27536902\n",
      " -0.2736017   0.04292321 -0.19570887  0.04856534 -0.38526696  0.08130675\n",
      " -0.03371506  0.28666574]\n"
     ]
    }
   ],
   "source": [
    "# application of analogy to calculate Queen vector\n",
    "\n",
    "queen_vector = king_vector - man_vector + woman_vector\n",
    "print(len(queen_vector))\n",
    "print(queen_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1271a0d0-8881-4993-9e27-91de08a1fc3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2df2a9-1108-4697-b4cb-fac574e2161c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3494437c-29ba-4a93-8e97-e0a53045deef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eeda3d7-6743-4d11-a278-2e6f0e317ccc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95901671-4a31-40f1-a617-9c019dbb284d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
